{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9Mee834f7J637osHI6mpi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niiingleiii/ML-French-Text-Classification/blob/main/Nvidia_Main_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import standard libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "import ipywidgets as widgets\n",
        "from ipywidgets import interact, interact_manual\n",
        "import math\n",
        "from collections import defaultdict\n",
        "import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import for text analytics\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.model_selection import train_test_split\n",
        "import string\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import Doc2Vec\n",
        "from gensim.models.doc2vec import TaggedDocument\n",
        "from gensim.models import LdaModel, CoherenceModel, TfidfModel\n",
        "from gensim.corpora import Dictionary\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim import corpora\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from transformers import CamembertTokenizer, CamembertModel\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import FlaubertModel, FlaubertTokenizer\n",
        "!pip install nlpaug\n",
        "import nlpaug.augmenter.char as nac\n",
        "import nlpaug.augmenter.word as naw\n",
        "import nlpaug.augmenter.sentence as nas\n",
        "\n",
        "# Import for classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.model_selection import GridSearchCV, KFold, cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.metrics import confusion_matrix # Confusion matrix\n",
        "import joblib\n",
        "device = torch.device('mps')\n",
        "\n",
        "# Import for youtube data extraction and display\n",
        "from googleapiclient.discovery import build\n",
        "from google_auth_oauthlib.flow import InstalledAppFlow\n",
        "from google.auth.transport.requests import Request\n",
        "!pip install youtube_transcript_api\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "from IPython.display import Image, display, HTML\n",
        "import re\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "# Import for social network analysis\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "vgQV0D7LNrhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA\n",
        "\n"
      ],
      "metadata": {
        "id": "rAq-Bsd_Nn_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('https://raw.githubusercontent.com/Niiingleiii/ML-data/main/training_data.csv')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/Niiingleiii/ML-data/main/unlabelled_test_data.csv')"
      ],
      "metadata": {
        "id": "b0dXoTksNy-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(train_ori)\n",
        "display(test)"
      ],
      "metadata": {
        "id": "CmBnXsA7NzX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new column to store the length of the texts\n",
        "train['sentence length'] = train['sentence'].apply(lambda x: len(str(x)))\n",
        "train"
      ],
      "metadata": {
        "id": "hQzfh86AN1Cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregating the mean length of sentences per difficulty\n",
        "train_mean_length = (\n",
        "    train\n",
        "    .groupby(['difficulty'])[['sentence length']]\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    .rename(columns={'sentence length': 'mean length'})\n",
        ")\n",
        "\n",
        "# Aggregating the count of sentences per difficulty\n",
        "train_count = (\n",
        "    train\n",
        "    .groupby(['difficulty'])[['sentence']]\n",
        "    .count()\n",
        "    .reset_index()\n",
        "    .rename(columns={'sentence': 'count'})\n",
        ")\n",
        "\n",
        "# Merging both aggregates into a single DataFrame\n",
        "train_eda = train_mean_length.merge(train_count, on='difficulty')\n",
        "train_eda\n"
      ],
      "metadata": {
        "id": "GufTs1YtN7V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw the bar chart of the count of texts in each difficulty\n",
        "plt.bar(train_eda['difficulty'], train_eda['count'])\n",
        "plt.title('Bar Chart of the Count of Texts in Each Difficulty')\n",
        "plt.xlabel('difficulty')\n",
        "plt.ylabel('count of texts')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xerB2MB2N8u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw the bar chart of the mean length of texts in each difficulty\n",
        "plt.bar(train_eda['difficulty'], train_eda['mean length'])\n",
        "\n",
        "plt.title('Bar Chart of the Mean Length of Texts in Each Difficulty')\n",
        "plt.xlabel('difficulty')\n",
        "plt.ylabel('count of texts')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AZe6GT1LN-Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw the box plot\n",
        "sns.boxplot(x='difficulty', y='sentence length', data=train, order=['A1', 'A2', 'B1', 'B2', 'C1', 'C2'])\n",
        "\n",
        "plt.title(\"Box Plot of the Distribution of Sentence Length in Each Difficulty\")\n",
        "plt.xlabel(\"difficulty\")\n",
        "plt.ylabel(\"sentence length\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KZYpBk3vN_iZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Models: Logistic Regression, KNN, Random Forest, Decision Tree"
      ],
      "metadata": {
        "id": "vbFL8Zg1GgpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0eFECC4GtOq",
        "outputId": "d5f5e41f-f2c8-49b0-b353-210d73c27b04"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 evaluate-0.4.2 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# Set environment variables for offline mode\n",
        "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
        "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n"
      ],
      "metadata": {
        "id": "0CvUC6t2Gtuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/Niiingleiii/ML-data/main/training_data.csv')\n",
        "aug_dataset = pd.read_csv('https://raw.githubusercontent.com/emilysr2/Data-Science-and-Machine-Learning/main/augmented_training_data_7000_v3%20(1).csv')\n",
        "dataset = pd.concat([dataset, aug_dataset], ignore_index=True)\n",
        "dataset['difficulty'] = dataset['difficulty'].astype('category')\n",
        "class_names = dataset['difficulty'].cat.categories.tolist()\n",
        "\n",
        "# Split dataset\n",
        "train_set, validation_set = train_test_split(dataset, test_size=0.1, random_state=42)\n",
        "\n",
        "# Number of labels\n",
        "num_labels = train_set['difficulty'].nunique()\n",
        "\n",
        "# Convert 'difficulty' to an integer\n",
        "train_set['difficulty'] = train_set['difficulty'].cat.codes\n",
        "validation_set['difficulty'] = validation_set['difficulty'].cat.codes\n",
        "\n",
        "# Fill any missing values\n",
        "train_set = train_set.fillna('')\n",
        "validation_set = validation_set.fillna('')\n",
        "\n",
        "# Rename columns to match expected feature names\n",
        "train_set = train_set.rename(columns={'sentence': 'text', 'difficulty': 'labels'})\n",
        "validation_set = validation_set.rename(columns={'sentence': 'text', 'difficulty': 'labels'})\n",
        "\n",
        "# Select only the columns needed for the Dataset\n",
        "train_set = train_set[['text', 'labels']]\n",
        "validation_set = validation_set[['text', 'labels']]\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = CamembertTokenizer.from_pretrained('almanach/camembert-base')\n",
        "\n",
        "# Preprocess data\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "# Apply preprocessing to training data\n",
        "train_encodings = preprocess_function({'text': train_set['text'].tolist()})\n",
        "val_encodings = preprocess_function({'text': validation_set['text'].tolist()})\n",
        "\n",
        "X_train = train_encodings['input_ids']\n",
        "X_val = val_encodings['input_ids']\n",
        "y_train = train_set['labels'].tolist()\n",
        "y_val = validation_set['labels'].tolist()\n"
      ],
      "metadata": {
        "id": "QOozttwrGwey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    model_accuracy = accuracy_score(y_test, y_pred)\n",
        "    model_precision = precision_score(y_test, y_pred, average='macro')\n",
        "    model_recall = recall_score(y_test, y_pred, average='macro')\n",
        "    model_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    model_eval = [model_accuracy, model_precision, model_recall, model_f1]\n",
        "    return model_eval"
      ],
      "metadata": {
        "id": "DmxrgRkeGy4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize\n",
        "logistic_regression_model = LogisticRegression(max_iter=1000)\n",
        "knn_model = KNeighborsClassifier(algorithm='kd_tree')\n",
        "decision_tree_model = DecisionTreeClassifier()\n",
        "random_forest_model = RandomForestClassifier()\n",
        "\n",
        "# train\n",
        "logistic_regression_model.fit(X_train, y_train)\n",
        "knn_model.fit(X_train, y_train)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "# results\n",
        "model_comparison = pd.DataFrame(index=['Accuracy', 'Precision', 'Recall', 'F1'])\n",
        "\n",
        "model_comparison['Logistic Regression'] = evaluation(logistic_regression_model, X_val, y_val)\n",
        "model_comparison['KNN'] = evaluation(knn_model, X_val, y_val)\n",
        "model_comparison['Decision Tree'] = evaluation(decision_tree_model, X_val, y_val)\n",
        "model_comparison['Random Forest'] = evaluation(random_forest_model, X_val, y_val)\n",
        "\n",
        "print(model_comparison)"
      ],
      "metadata": {
        "id": "9VCEy3dTG0qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc_model_best = RandomForestClassifier(max_depth=None, min_samples_split=2, n_estimators=300, random_state=42)\n",
        "rfc_model_best.fit(X_train,y_train)\n",
        "model_comparison['Random Forest (best parameter)'] = evaluation(rfc_model_best, X_val, y_val)\n",
        "\n",
        "print(model_comparison)"
      ],
      "metadata": {
        "id": "AdDO164bHSMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Models: LGBM, Random Trees, XGBoost, CatBoost"
      ],
      "metadata": {
        "id": "YoDudeHfHDJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "etc_model = ExtraTreesClassifier(random_state=42)\n",
        "etc_model.fit(X_train,y_train)\n",
        "\n",
        "model_comparison['Extra Trees'] = evaluation(etc_model, X_val, y_val)\n",
        "model_comparison"
      ],
      "metadata": {
        "id": "OyEtGjChHS0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "lgb_model = LGBMClassifier()\n",
        "lgb_model.fit(X_train,y_train)\n",
        "\n",
        "model_comparison['LightGBM'] = evaluation(lgb_model, X_val, y_val)\n",
        "model_comparison"
      ],
      "metadata": {
        "id": "SgGk81_3HUfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb_model = XGBClassifier()\n",
        "xgb_model.fit(X_train,y_train)\n",
        "\n",
        "model_comparison['XGBoost'] = evaluation(xgb_model, X_val, y_val)\n",
        "model_comparison"
      ],
      "metadata": {
        "id": "ZWWhAuIWHXf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier"
      ],
      "metadata": {
        "id": "ga_b32jZHX2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cb_model_bert = CatBoostClassifier()\n",
        "cb_model_bert.fit(X_train,y_train)\n",
        "\n",
        "model_comparison['Catboost'] = evaluation(cb_model_bert, X_val, y_val)\n",
        "model_comparison"
      ],
      "metadata": {
        "id": "vwqUYZc0HZy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Textural Features"
      ],
      "metadata": {
        "id": "16e2Kw-2OdJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a copy of training and testing dataset\n",
        "train_original = train.copy()\n",
        "test_original = test.copy()"
      ],
      "metadata": {
        "id": "46QZbsylOfOO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create X and y for the model training\n",
        "X = train['sentence']\n",
        "y = train['difficulty']"
      ],
      "metadata": {
        "id": "c8NFNMQWOiVx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##num_characters and average_length"
      ],
      "metadata": {
        "id": "dtdm6bMfOlt8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate num_characters and average_length\n",
        "train_original['num_characters'] = train_original['sentence'].apply(len)\n",
        "test_original['num_characters'] = test_original['sentence'].apply(len)\n",
        "\n",
        "def avg_word_length(sentence):\n",
        "    words = sentence.split()\n",
        "    if len(words) == 0:\n",
        "        return 0\n",
        "    return sum(len(word) for word in words) / len(words)\n",
        "\n",
        "train_original['average_length'] = train_original['sentence'].apply(avg_word_length)\n",
        "test_original['average_length'] = test_original['sentence'].apply(avg_word_length)"
      ],
      "metadata": {
        "id": "IO7xy9ILOjkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Lexical Richness Metrics"
      ],
      "metadata": {
        "id": "TCMxBZnQOpiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lexical_metrics(text):\n",
        "    tokens = text.split()\n",
        "    MTLD = ld.mtld(tokens)\n",
        "    return MTLD\n",
        "\n",
        "train_original['TTR'], train_original['MSTTR'], train_original['MATTR'], train_original['MTLD'] = zip(*train_original['sentence'].apply(lexical_metrics))\n",
        "test_original['TTR'], test_original['MSTTR'], test_original['MATTR'], test_original['MTLD'] = zip(*test_original['sentence'].apply(lexical_metrics))"
      ],
      "metadata": {
        "id": "fqsfqfUROrrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vocabulary Complexity Metrics"
      ],
      "metadata": {
        "id": "XmWqNhsoOttB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq = {'le': 1000, 'chat': 500, 'est': 800, 'noir': 200}  # This should be a comprehensive dictionary\n",
        "\n",
        "def average_word_frequency(sentence, word_freq):\n",
        "    words = sentence.split()\n",
        "    frequencies = [word_freq.get(word.lower(), 0) for word in words]\n",
        "    return np.mean(frequencies) if frequencies else 0\n",
        "\n",
        "def percent_words_not_in_list(sentence, simple_word_list):\n",
        "    words = sentence.split()\n",
        "    not_in_list = [word for word in words if word.lower() not in simple_word_list]\n",
        "    return len(not_in_list) / len(words)\n",
        "\n",
        "simple_word_list = set(word_freq.keys())  # Example simple word list\n",
        "\n",
        "train_original['average_word_frequency'] = train_original['sentence'].apply(lambda x: average_word_frequency(x, word_freq))\n",
        "test_original['average_word_frequency'] = test_original['sentence'].apply(lambda x: average_word_frequency(x, word_freq))\n",
        "train_original['percent_words_not_in_list'] = train_original['sentence'].apply(lambda x: percent_words_not_in_list(x, simple_word_list))\n",
        "test_original['percent_words_not_in_list'] = test_original['sentence'].apply(lambda x: percent_words_not_in_list(x, simple_word_list))"
      ],
      "metadata": {
        "id": "9M3heZJ1Ov76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Syntactic Complexity Metrics"
      ],
      "metadata": {
        "id": "lkDPjzwbO9nR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def syntactic_metrics(text):\n",
        "    doc = nlp(text)\n",
        "    sentences = list(doc.sents)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(doc)\n",
        "    num_clauses = sum([1 for token in doc if token.dep_ in [\"ccomp\", \"advcl\", \"relcl\", \"xcomp\", \"acl\"]])\n",
        "    mean_length_sent = num_words / num_sentences if num_sentences > 0 else 0\n",
        "    mean_length_clause = num_words / num_clauses if num_clauses > 0 else 0\n",
        "    t_units = [sent for sent in sentences if any([token.dep_ == 'ROOT' for token in sent])]\n",
        "    complex_t_units = [t for t in t_units if len([token for token in t if token.dep_ in [\"ccomp\", \"advcl\", \"relcl\", \"xcomp\", \"acl\"]]) > 0]\n",
        "    mean_length_t_unit = num_words / len(t_units) if len(t_units) > 0 else 0\n",
        "    return mean_length_sent, mean_length_clause, len(complex_t_units) / len(t_units) if len(t_units) > 0 else 0\n",
        "\n",
        "train_original['MLS'], train_original['MLC'], train_original['complex_t_units_ratio'] = zip(*train_original['sentence'].apply(syntactic_metrics))\n",
        "test_original['MLS'], test_original['MLC'], test_original['complex_t_units_ratio'] = zip(*test_original['sentence'].apply(syntactic_metrics))"
      ],
      "metadata": {
        "id": "XHKHYhpAPAjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## POS Tagging"
      ],
      "metadata": {
        "id": "LzoJkU31PCQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS\n",
        "def extract_pos_tags(sentence):\n",
        "    doc = nlp(sentence)\n",
        "    pos_tags = [token.pos_ for token in doc]\n",
        "    return pos_tags\n",
        "\n",
        "def pos_features(sentences):\n",
        "    pos_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n",
        "    features = []\n",
        "    for sentence in sentences:\n",
        "        pos_tags = extract_pos_tags(sentence)\n",
        "        pos_count = {pos: 0 for pos in pos_list}\n",
        "        for tag in pos_tags:\n",
        "            if tag in pos_count:\n",
        "                pos_count[tag] += 1\n",
        "        features.append([pos_count[pos] for pos in pos_list])\n",
        "    return features\n",
        "\n",
        "train_pos_features = pos_features(train_original['sentence'])\n",
        "test_pos_features = pos_features(test_original['sentence'])\n",
        "\n",
        "pos_columns = [f'POS_{pos}' for pos in ['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']]\n",
        "df_train_pos_features = pd.DataFrame(train_pos_features, columns=pos_columns)\n",
        "df_test_pos_features = pd.DataFrame(test_pos_features, columns=pos_columns)\n"
      ],
      "metadata": {
        "id": "c8VDP36cPDiN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Readability Scores"
      ],
      "metadata": {
        "id": "rK50DSS4PFd1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def kandel_moles_score(sentence):\n",
        "    num_sentences = len(list(nlp(sentence).sents))\n",
        "    num_syllables = sum([textstat.syllable_count(word) for word in sentence.split()])\n",
        "    num_words = len(sentence.split())\n",
        "    if num_sentences == 0 or num_words == 0:\n",
        "        return 0\n",
        "    return 207 - 1.015 * (num_words / num_sentences) - 0.736 * (num_syllables / num_words)\n",
        "\n",
        "def bingu_score(sentence):\n",
        "    return sentence.count(',') + sentence.count(';') + sentence.count(':') + sentence.count('(') + sentence.count(')')\n",
        "\n",
        "train_original['KM_score'] = train_original['sentence'].apply(kandel_moles_score)\n",
        "test_original['KM_score'] = test_original['sentence'].apply(kandel_moles_score)\n",
        "train_original['BINGUI'] = train_original['sentence'].apply(bingu_score)\n",
        "test_original['BINGUI'] = test_original['sentence'].apply(bingu_score)"
      ],
      "metadata": {
        "id": "gnmYBuuaPHp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our Best Model - CamenBert"
      ],
      "metadata": {
        "id": "EMHVmffIEn2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall accelerate -y\n",
        "# Install necessary packages\n",
        "!pip install transformers datasets evaluate accelerate -U"
      ],
      "metadata": {
        "id": "HpLYetCiDmKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XISYQmmQDICY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset, Features, ClassLabel, Value\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to data\n",
        "train_data_path = 'https://raw.githubusercontent.com/Niiingleiii/ML-data/main/training_data.csv'\n",
        "aug_data_path = 'https://raw.githubusercontent.com/emilysr2/Data-Science-and-Machine-Learning/main/augmented_training_data_7000_v3%20(1).csv'\n",
        "test_data_path = 'https://raw.githubusercontent.com/Niiingleiii/ML-data/main/unlabelled_test_data.csv'\n",
        "\n",
        "# Load dataset\n",
        "dataset = pd.read_csv(train_data_path)\n",
        "aug_dataset = pd.read_csv(aug_data_path)\n",
        "dataset = pd.concat([dataset, aug_dataset], ignore_index=True)\n",
        "dataset['difficulty'] = dataset['difficulty'].astype('category')\n",
        "class_names = dataset['difficulty'].cat.categories.tolist()\n",
        "\n",
        "print(f'Number of datapoints: {len(dataset)}')\n",
        "print(f'Datatable:\\n {dataset.head(4)}')\n",
        "print(f'Datatable columns {dataset.columns}')"
      ],
      "metadata": {
        "id": "sUPQ6_DsDrWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rain_set, validation_set = train_test_split(dataset, test_size=0.1, random_state=42)\n",
        "\n",
        "# Print the size of each set\n",
        "print(f'Number of training datapoints: {len(train_set)}')\n",
        "print(f'Number of validation datapoints: {len(validation_set)}')\n",
        "\n",
        "# Optionally, print some samples from each set\n",
        "print(f'Training datatable sample:\\n {train_set.head(3)}')\n",
        "print(f'Validation datatable sample:\\n {validation_set.head(3)}')\n",
        "\n",
        "# Number of labels\n",
        "num_labels = train_set['difficulty'].nunique()\n",
        "print(f'Number of labels: {num_labels}')\n",
        "\n",
        "# Ensure data types are supported\n",
        "train_set['difficulty'] = train_set['difficulty'].cat.codes\n",
        "validation_set['difficulty'] = validation_set['difficulty'].cat.codes\n",
        "\n",
        "# Fill any missing values\n",
        "train_set = train_set.fillna('')\n",
        "validation_set = validation_set.fillna('')\n",
        "\n",
        "# Rename columns to match expected feature names\n",
        "train_set = train_set.rename(columns={'sentence': 'text', 'difficulty': 'labels'})\n",
        "validation_set = validation_set.rename(columns={'sentence': 'text', 'difficulty': 'labels'})\n",
        "\n",
        "# Select only the columns needed for the Dataset\n",
        "train_set = train_set[['text', 'labels']]\n",
        "validation_set = validation_set[['text', 'labels']]\n",
        "\n",
        "# Define feature types explicitly\n",
        "features = Features({\n",
        "    'text': Value('string'),\n",
        "    'labels': ClassLabel(num_classes=num_labels)\n",
        "})"
      ],
      "metadata": {
        "id": "zdSIEMHXDsVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "# Freeze all layers except the last two transformer layers and the classification head\n",
        "def freeze_layers(model):\n",
        "    for param in model.roberta.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Access the last layer of the encoder\n",
        "    last_layer = model.roberta.encoder.layer[-1:]\n",
        "    # Enable gradient computation for the parameters in the last layer\n",
        "    for param in last_layer.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for param in model.classifier.parameters():\n",
        "        param.requires_grad = True\n"
      ],
      "metadata": {
        "id": "Aea2movhEU0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess data\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "# Create Hugging Face Datasets\n",
        "train_dataset = Dataset.from_pandas(train_set, features=features, preserve_index=False)\n",
        "val_dataset = Dataset.from_pandas(validation_set, features=features, preserve_index=False)\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy_metric = evaluate.load('accuracy')\n",
        "precision_metric = evaluate.load('precision')\n",
        "recall_metric = evaluate.load('recall')\n",
        "f1_metric = evaluate.load('f1')\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    labels = p.label_ids\n",
        "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)\n",
        "    precision = precision_metric.compute(predictions=preds, references=labels, average='weighted')\n",
        "    recall = recall_metric.compute(predictions=preds, references=labels, average='weighted')\n",
        "    f1 = f1_metric.compute(predictions=preds, references=labels, average='weighted')\n",
        "    return {\n",
        "        'eval_accuracy': accuracy['accuracy'],\n",
        "        'eval_precision': precision['precision'],\n",
        "        'eval_recall': recall['recall'],\n",
        "        'eval_f1': f1['f1']\n",
        "    }"
      ],
      "metadata": {
        "id": "oYyLhlFvEWzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration for the best model\n",
        "best_training_config = {'seed': 42, 'learning_rate': 10e-5}\n",
        "\n",
        "# Train the best model\n",
        "best_model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=num_labels)\n",
        "freeze_layers(best_model)\n",
        "\n",
        "best_training_args = TrainingArguments(\n",
        "    output_dir=f'./results_seed_{best_training_config[\"seed\"]}',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=best_training_config['learning_rate'],\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=6,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=f'./logs_seed_{best_training_config[\"seed\"]}',\n",
        "    logging_steps=15,\n",
        "    seed=best_training_config['seed']\n",
        ")\n",
        "\n",
        "best_trainer = Trainer(\n",
        "    model=best_model,\n",
        "    args=best_training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "best_trainer.train()\n",
        "\n",
        "# Evaluate the best model\n",
        "best_metrics = best_trainer.evaluate()\n",
        "print(\"Metrics for the best model:\")\n",
        "print(f\"Accuracy: {best_metrics['eval_accuracy']}\")\n",
        "print(f\"Precision: {best_metrics['eval_precision']}\")\n",
        "print(f\"Recall: {best_metrics['eval_recall']}\")\n",
        "print(f\"F1 Score: {best_metrics['eval_f1']}\")\n"
      ],
      "metadata": {
        "id": "08S9E81KEZHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "# Evaluate and get predictions\n",
        "predictions = trainer.predict(val_dataset)\n",
        "preds = predictions.predictions.argmax(-1)\n",
        "labels = predictions.label_ids\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(labels, preds)\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iOGb4sPGIINk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensembling Using CamenBert"
      ],
      "metadata": {
        "id": "WOlPGeCvEwO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " Training configurations for ensembling\n",
        "ensemble_training_configs = [\n",
        "    {'seed': 42, 'learning_rate': 10e-5},\n",
        "    {'seed': 43, 'learning_rate': 11e-5},\n",
        "    {'seed': 44, 'learning_rate': 10e-5},\n",
        "    {'seed': 45, 'learning_rate': 10e-5},\n",
        "    {'seed': 46, 'learning_rate': 1e-5},\n",
        "]\n",
        "\n",
        "ensemble_models = []\n",
        "ensemble_trainers = []\n",
        "\n",
        "# Train the models for ensembling\n",
        "for config in ensemble_training_configs:\n",
        "    model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=num_labels)\n",
        "    freeze_layers(model)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results_seed_{config[\"seed\"]}',\n",
        "        evaluation_strategy='epoch',\n",
        "        learning_rate=config['learning_rate'],\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=6,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f'./logs_seed_{config[\"seed\"]}',\n",
        "        logging_steps=15,\n",
        "        seed=config['seed']\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    ensemble_trainers.append(trainer)\n",
        "    ensemble_models.append(model)"
      ],
      "metadata": {
        "id": "ZX9fvheiEbDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the ensemble model\n",
        "ensemble_predictions = []\n",
        "for trainer in ensemble_trainers:\n",
        "    predictions = trainer.predict(val_dataset)\n",
        "    ensemble_predictions.append(predictions.predictions)\n",
        "\n",
        "# Average the predictions\n",
        "average_val_predictions = np.mean(np.stack(ensemble_predictions), axis=0)\n",
        "ensemble_preds = np.argmax(average_val_predictions, axis=1)\n",
        "\n",
        "# Calculate ensemble metrics\n",
        "ensemble_labels = val_dataset['labels']\n",
        "ensemble_accuracy = accuracy_score(ensemble_labels, ensemble_preds)\n",
        "ensemble_precision = precision_score(ensemble_labels, ensemble_preds, average='weighted')\n",
        "ensemble_recall = recall_score(ensemble_labels, ensemble_preds, average='weighted')\n",
        "ensemble_f1 = f1_score(ensemble_labels, ensemble_preds, average='weighted')\n",
        "\n",
        "print(\"Metrics for the ensemble model:\")\n",
        "print(f\"Accuracy: {ensemble_accuracy}\")\n",
        "print(f\"Precision: {ensemble_precision}\")\n",
        "print(f\"Recall: {ensemble_recall}\")\n",
        "print(f\"F1 Score: {ensemble_f1}\")"
      ],
      "metadata": {
        "id": "DpG_jY-TEgY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the models and make predictions for ensembling\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "test_data = test_data.rename(columns={'sentence': 'text'})\n",
        "id = test_data[['id']]\n",
        "test_data = test_data[['text']]\n",
        "test_dataset = Dataset.from_pandas(test_data, preserve_index=False)\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "# Collect predictions from each model\n",
        "all_predictions = []\n",
        "\n",
        "for trainer in ensemble_trainers:\n",
        "    predictions = trainer.predict(test_dataset)\n",
        "    all_predictions.append(predictions.predictions)\n",
        "\n",
        "# Average the predictions to get the final prediction\n",
        "average_predictions = np.mean(np.stack(all_predictions), axis=0)\n",
        "final_preds = np.argmax(average_predictions, axis=1)\n",
        "\n",
        "# Map the predictions to the original labels\n",
        "pred_labels = [class_names[pred] for pred in final_preds]\n",
        "\n",
        "# Save the predictions to a submission file\n",
        "submission = pd.DataFrame({'id': id['id'].values.tolist(), 'difficulty': pred_labels})\n",
        "submission.to_csv('/content/submission_ensemble.csv', index=False)\n",
        "\n",
        "print(\"Ensemble predictions saved to submission_ensemble.csv\")\n",
        "\n",
        "# Code to download the file in Google Colab\n",
        "from google.colab import files\n",
        "files.download('/content/submission_ensemble.csv')"
      ],
      "metadata": {
        "id": "DciiGdKGEidw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}