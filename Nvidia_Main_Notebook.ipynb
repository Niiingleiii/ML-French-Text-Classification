{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlQnOakk7z4PiA8cyyYcTy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niiingleiii/ML-French-Text-Classification/blob/main/Nvidia_Main_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic Models: Logistic Regression, KNN, Random Forest, Decision Tree"
      ],
      "metadata": {
        "id": "vbFL8Zg1GgpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n0eFECC4GtOq",
        "outputId": "d5f5e41f-f2c8-49b0-b353-210d73c27b04"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.19.1-py3-none-any.whl (542 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m542.0/542.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets, evaluate\n",
            "Successfully installed datasets-2.19.1 dill-0.3.8 evaluate-0.4.2 multiprocess-0.70.16 xxhash-3.4.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import evaluate\n",
        "import numpy as np\n",
        "\n",
        "# Set environment variables for offline mode\n",
        "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
        "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n"
      ],
      "metadata": {
        "id": "0CvUC6t2Gtuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "dataset = pd.read_csv('https://raw.githubusercontent.com/Niiingleiii/ML-data/main/training_data.csv')\n",
        "aug_dataset = pd.read_csv('https://raw.githubusercontent.com/emilysr2/Data-Science-and-Machine-Learning/main/augmented_training_data_7000_v3%20(1).csv')\n",
        "dataset = pd.concat([dataset, aug_dataset], ignore_index=True)\n",
        "dataset['difficulty'] = dataset['difficulty'].astype('category')\n",
        "class_names = dataset['difficulty'].cat.categories.tolist()\n",
        "\n",
        "# Split dataset\n",
        "train_set, validation_set = train_test_split(dataset, test_size=0.1, random_state=42)\n",
        "\n",
        "# Number of labels\n",
        "num_labels = train_set['difficulty'].nunique()\n",
        "\n",
        "# Convert 'difficulty' to an integer\n",
        "train_set['difficulty'] = train_set['difficulty'].cat.codes\n",
        "validation_set['difficulty'] = validation_set['difficulty'].cat.codes\n",
        "\n",
        "# Fill any missing values\n",
        "train_set = train_set.fillna('')\n",
        "validation_set = validation_set.fillna('')\n",
        "\n",
        "# Rename columns to match expected feature names\n",
        "train_set = train_set.rename(columns={'sentence': 'text', 'difficulty': 'labels'})\n",
        "validation_set = validation_set.rename(columns={'sentence': 'text', 'difficulty': 'labels'})\n",
        "\n",
        "# Select only the columns needed for the Dataset\n",
        "train_set = train_set[['text', 'labels']]\n",
        "validation_set = validation_set[['text', 'labels']]\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = CamembertTokenizer.from_pretrained('almanach/camembert-base')\n",
        "\n",
        "# Preprocess data\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "# Apply preprocessing to training data\n",
        "train_encodings = preprocess_function({'text': train_set['text'].tolist()})\n",
        "val_encodings = preprocess_function({'text': validation_set['text'].tolist()})\n",
        "\n",
        "X_train = train_encodings['input_ids']\n",
        "X_val = val_encodings['input_ids']\n",
        "y_train = train_set['labels'].tolist()\n",
        "y_val = validation_set['labels'].tolist()\n"
      ],
      "metadata": {
        "id": "QOozttwrGwey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluation(model, X_test, y_test):\n",
        "    y_pred = model.predict(X_test)\n",
        "    model_accuracy = accuracy_score(y_test, y_pred)\n",
        "    model_precision = precision_score(y_test, y_pred, average='macro')\n",
        "    model_recall = recall_score(y_test, y_pred, average='macro')\n",
        "    model_f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    model_eval = [model_accuracy, model_precision, model_recall, model_f1]\n",
        "    return model_eval"
      ],
      "metadata": {
        "id": "DmxrgRkeGy4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize\n",
        "logistic_regression_model = LogisticRegression(max_iter=1000)\n",
        "knn_model = KNeighborsClassifier(algorithm='kd_tree')\n",
        "decision_tree_model = DecisionTreeClassifier()\n",
        "random_forest_model = RandomForestClassifier()\n",
        "\n",
        "# train\n",
        "logistic_regression_model.fit(X_train, y_train)\n",
        "knn_model.fit(X_train, y_train)\n",
        "decision_tree_model.fit(X_train, y_train)\n",
        "random_forest_model.fit(X_train, y_train)\n",
        "\n",
        "# results\n",
        "model_comparison = pd.DataFrame(index=['Accuracy', 'Precision', 'Recall', 'F1'])\n",
        "\n",
        "model_comparison['Logistic Regression'] = evaluation(logistic_regression_model, X_val, y_val)\n",
        "model_comparison['KNN'] = evaluation(knn_model, X_val, y_val)\n",
        "model_comparison['Decision Tree'] = evaluation(decision_tree_model, X_val, y_val)\n",
        "model_comparison['Random Forest'] = evaluation(random_forest_model, X_val, y_val)\n",
        "\n",
        "print(model_comparison)"
      ],
      "metadata": {
        "id": "9VCEy3dTG0qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rfc_model_best = RandomForestClassifier(max_depth=None, min_samples_split=2, n_estimators=300, random_state=42)\n",
        "rfc_model_best.fit(X_train,y_train)\n",
        "model_comparison['Random Forest (best parameter)'] = evaluation(rfc_model_best, X_val, y_val)\n",
        "\n",
        "print(model_comparison)"
      ],
      "metadata": {
        "id": "AdDO164bHSMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Models: LGBM, Random Trees, XGBoost, CatBoost"
      ],
      "metadata": {
        "id": "YoDudeHfHDJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "etc_model = ExtraTreesClassifier(random_state=42)\n",
        "etc_model.fit(X_train,y_train)\n",
        "\n",
        "model_comparison['Extra Trees'] = evaluation(etc_model, X_val, y_val)\n",
        "model_comparison"
      ],
      "metadata": {
        "id": "OyEtGjChHS0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "lgb_model = LGBMClassifier()\n",
        "lgb_model.fit(X_train,y_train)\n",
        "\n",
        "model_comparison['LightGBM'] = evaluation(lgb_model, X_val, y_val)\n",
        "model_comparison"
      ],
      "metadata": {
        "id": "SgGk81_3HUfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier\n",
        "xgb_model = XGBClassifier()\n",
        "xgb_model.fit(X_train,y_train)\n",
        "\n",
        "model_comparison['XGBoost'] = evaluation(xgb_model, X_val, y_val)\n",
        "model_comparison"
      ],
      "metadata": {
        "id": "ZWWhAuIWHXf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier"
      ],
      "metadata": {
        "id": "ga_b32jZHX2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cb_model_bert = CatBoostClassifier()\n",
        "cb_model_bert.fit(X_train,y_train)\n",
        "\n",
        "model_comparison['Catboost'] = evaluation(cb_model_bert, X_val, y_val)\n",
        "model_comparison"
      ],
      "metadata": {
        "id": "vwqUYZc0HZy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Our Best Model - CamenBert"
      ],
      "metadata": {
        "id": "EMHVmffIEn2n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall accelerate -y\n",
        "# Install necessary packages\n",
        "!pip install transformers datasets evaluate accelerate -U"
      ],
      "metadata": {
        "id": "HpLYetCiDmKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XISYQmmQDICY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import CamembertTokenizer, CamembertForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset, Features, ClassLabel, Value\n",
        "import evaluate\n",
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, accuracy_score, f1_score, recall_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to data\n",
        "train_data_path = 'https://raw.githubusercontent.com/Niiingleiii/ML-data/main/training_data.csv'\n",
        "aug_data_path = 'https://raw.githubusercontent.com/emilysr2/Data-Science-and-Machine-Learning/main/augmented_training_data_7000_v3%20(1).csv'\n",
        "test_data_path = 'https://raw.githubusercontent.com/Niiingleiii/ML-data/main/unlabelled_test_data.csv'\n",
        "\n",
        "# Load dataset\n",
        "dataset = pd.read_csv(train_data_path)\n",
        "aug_dataset = pd.read_csv(aug_data_path)\n",
        "dataset = pd.concat([dataset, aug_dataset], ignore_index=True)\n",
        "dataset['difficulty'] = dataset['difficulty'].astype('category')\n",
        "class_names = dataset['difficulty'].cat.categories.tolist()\n",
        "\n",
        "print(f'Number of datapoints: {len(dataset)}')\n",
        "print(f'Datatable:\\n {dataset.head(4)}')\n",
        "print(f'Datatable columns {dataset.columns}')"
      ],
      "metadata": {
        "id": "sUPQ6_DsDrWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rain_set, validation_set = train_test_split(dataset, test_size=0.1, random_state=42)\n",
        "\n",
        "# Print the size of each set\n",
        "print(f'Number of training datapoints: {len(train_set)}')\n",
        "print(f'Number of validation datapoints: {len(validation_set)}')\n",
        "\n",
        "# Optionally, print some samples from each set\n",
        "print(f'Training datatable sample:\\n {train_set.head(3)}')\n",
        "print(f'Validation datatable sample:\\n {validation_set.head(3)}')\n",
        "\n",
        "# Number of labels\n",
        "num_labels = train_set['difficulty'].nunique()\n",
        "print(f'Number of labels: {num_labels}')\n",
        "\n",
        "# Ensure data types are supported\n",
        "train_set['difficulty'] = train_set['difficulty'].cat.codes\n",
        "validation_set['difficulty'] = validation_set['difficulty'].cat.codes\n",
        "\n",
        "# Fill any missing values\n",
        "train_set = train_set.fillna('')\n",
        "validation_set = validation_set.fillna('')\n",
        "\n",
        "# Rename columns to match expected feature names\n",
        "train_set = train_set.rename(columns={'sentence': 'text', 'difficulty': 'labels'})\n",
        "validation_set = validation_set.rename(columns={'sentence': 'text', 'difficulty': 'labels'})\n",
        "\n",
        "# Select only the columns needed for the Dataset\n",
        "train_set = train_set[['text', 'labels']]\n",
        "validation_set = validation_set[['text', 'labels']]\n",
        "\n",
        "# Define feature types explicitly\n",
        "features = Features({\n",
        "    'text': Value('string'),\n",
        "    'labels': ClassLabel(num_classes=num_labels)\n",
        "})"
      ],
      "metadata": {
        "id": "zdSIEMHXDsVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "tokenizer = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "# Freeze all layers except the last two transformer layers and the classification head\n",
        "def freeze_layers(model):\n",
        "    for param in model.roberta.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Access the last layer of the encoder\n",
        "    last_layer = model.roberta.encoder.layer[-1:]\n",
        "    # Enable gradient computation for the parameters in the last layer\n",
        "    for param in last_layer.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    for param in model.classifier.parameters():\n",
        "        param.requires_grad = True\n"
      ],
      "metadata": {
        "id": "Aea2movhEU0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess data\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "# Create Hugging Face Datasets\n",
        "train_dataset = Dataset.from_pandas(train_set, features=features, preserve_index=False)\n",
        "val_dataset = Dataset.from_pandas(validation_set, features=features, preserve_index=False)\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Evaluation metrics\n",
        "accuracy_metric = evaluate.load('accuracy')\n",
        "precision_metric = evaluate.load('precision')\n",
        "recall_metric = evaluate.load('recall')\n",
        "f1_metric = evaluate.load('f1')\n",
        "\n",
        "def compute_metrics(p):\n",
        "    preds = p.predictions.argmax(-1)\n",
        "    labels = p.label_ids\n",
        "    accuracy = accuracy_metric.compute(predictions=preds, references=labels)\n",
        "    precision = precision_metric.compute(predictions=preds, references=labels, average='weighted')\n",
        "    recall = recall_metric.compute(predictions=preds, references=labels, average='weighted')\n",
        "    f1 = f1_metric.compute(predictions=preds, references=labels, average='weighted')\n",
        "    return {\n",
        "        'eval_accuracy': accuracy['accuracy'],\n",
        "        'eval_precision': precision['precision'],\n",
        "        'eval_recall': recall['recall'],\n",
        "        'eval_f1': f1['f1']\n",
        "    }"
      ],
      "metadata": {
        "id": "oYyLhlFvEWzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training configuration for the best model\n",
        "best_training_config = {'seed': 42, 'learning_rate': 10e-5}\n",
        "\n",
        "# Train the best model\n",
        "best_model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=num_labels)\n",
        "freeze_layers(best_model)\n",
        "\n",
        "best_training_args = TrainingArguments(\n",
        "    output_dir=f'./results_seed_{best_training_config[\"seed\"]}',\n",
        "    evaluation_strategy='epoch',\n",
        "    learning_rate=best_training_config['learning_rate'],\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=6,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=f'./logs_seed_{best_training_config[\"seed\"]}',\n",
        "    logging_steps=15,\n",
        "    seed=best_training_config['seed']\n",
        ")\n",
        "\n",
        "best_trainer = Trainer(\n",
        "    model=best_model,\n",
        "    args=best_training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "best_trainer.train()\n",
        "\n",
        "# Evaluate the best model\n",
        "best_metrics = best_trainer.evaluate()\n",
        "print(\"Metrics for the best model:\")\n",
        "print(f\"Accuracy: {best_metrics['eval_accuracy']}\")\n",
        "print(f\"Precision: {best_metrics['eval_precision']}\")\n",
        "print(f\"Recall: {best_metrics['eval_recall']}\")\n",
        "print(f\"F1 Score: {best_metrics['eval_f1']}\")\n"
      ],
      "metadata": {
        "id": "08S9E81KEZHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "# Evaluate and get predictions\n",
        "predictions = trainer.predict(val_dataset)\n",
        "preds = predictions.predictions.argmax(-1)\n",
        "labels = predictions.label_ids\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(labels, preds)\n",
        "\n",
        "# Display confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iOGb4sPGIINk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensembling Using CamenBert"
      ],
      "metadata": {
        "id": "WOlPGeCvEwO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " Training configurations for ensembling\n",
        "ensemble_training_configs = [\n",
        "    {'seed': 42, 'learning_rate': 10e-5},\n",
        "    {'seed': 43, 'learning_rate': 11e-5},\n",
        "    {'seed': 44, 'learning_rate': 10e-5},\n",
        "    {'seed': 45, 'learning_rate': 10e-5},\n",
        "    {'seed': 46, 'learning_rate': 1e-5},\n",
        "]\n",
        "\n",
        "ensemble_models = []\n",
        "ensemble_trainers = []\n",
        "\n",
        "# Train the models for ensembling\n",
        "for config in ensemble_training_configs:\n",
        "    model = CamembertForSequenceClassification.from_pretrained('camembert-base', num_labels=num_labels)\n",
        "    freeze_layers(model)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f'./results_seed_{config[\"seed\"]}',\n",
        "        evaluation_strategy='epoch',\n",
        "        learning_rate=config['learning_rate'],\n",
        "        per_device_train_batch_size=16,\n",
        "        per_device_eval_batch_size=16,\n",
        "        num_train_epochs=6,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir=f'./logs_seed_{config[\"seed\"]}',\n",
        "        logging_steps=15,\n",
        "        seed=config['seed']\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    ensemble_trainers.append(trainer)\n",
        "    ensemble_models.append(model)"
      ],
      "metadata": {
        "id": "ZX9fvheiEbDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the ensemble model\n",
        "ensemble_predictions = []\n",
        "for trainer in ensemble_trainers:\n",
        "    predictions = trainer.predict(val_dataset)\n",
        "    ensemble_predictions.append(predictions.predictions)\n",
        "\n",
        "# Average the predictions\n",
        "average_val_predictions = np.mean(np.stack(ensemble_predictions), axis=0)\n",
        "ensemble_preds = np.argmax(average_val_predictions, axis=1)\n",
        "\n",
        "# Calculate ensemble metrics\n",
        "ensemble_labels = val_dataset['labels']\n",
        "ensemble_accuracy = accuracy_score(ensemble_labels, ensemble_preds)\n",
        "ensemble_precision = precision_score(ensemble_labels, ensemble_preds, average='weighted')\n",
        "ensemble_recall = recall_score(ensemble_labels, ensemble_preds, average='weighted')\n",
        "ensemble_f1 = f1_score(ensemble_labels, ensemble_preds, average='weighted')\n",
        "\n",
        "print(\"Metrics for the ensemble model:\")\n",
        "print(f\"Accuracy: {ensemble_accuracy}\")\n",
        "print(f\"Precision: {ensemble_precision}\")\n",
        "print(f\"Recall: {ensemble_recall}\")\n",
        "print(f\"F1 Score: {ensemble_f1}\")"
      ],
      "metadata": {
        "id": "DpG_jY-TEgY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the models and make predictions for ensembling\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "test_data = test_data.rename(columns={'sentence': 'text'})\n",
        "id = test_data[['id']]\n",
        "test_data = test_data[['text']]\n",
        "test_dataset = Dataset.from_pandas(test_data, preserve_index=False)\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "# Collect predictions from each model\n",
        "all_predictions = []\n",
        "\n",
        "for trainer in ensemble_trainers:\n",
        "    predictions = trainer.predict(test_dataset)\n",
        "    all_predictions.append(predictions.predictions)\n",
        "\n",
        "# Average the predictions to get the final prediction\n",
        "average_predictions = np.mean(np.stack(all_predictions), axis=0)\n",
        "final_preds = np.argmax(average_predictions, axis=1)\n",
        "\n",
        "# Map the predictions to the original labels\n",
        "pred_labels = [class_names[pred] for pred in final_preds]\n",
        "\n",
        "# Save the predictions to a submission file\n",
        "submission = pd.DataFrame({'id': id['id'].values.tolist(), 'difficulty': pred_labels})\n",
        "submission.to_csv('/content/submission_ensemble.csv', index=False)\n",
        "\n",
        "print(\"Ensemble predictions saved to submission_ensemble.csv\")\n",
        "\n",
        "# Code to download the file in Google Colab\n",
        "from google.colab import files\n",
        "files.download('/content/submission_ensemble.csv')"
      ],
      "metadata": {
        "id": "DciiGdKGEidw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}